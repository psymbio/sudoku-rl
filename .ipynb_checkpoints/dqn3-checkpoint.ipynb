{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44fc8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tensorflow\n",
    "import gym\n",
    "import keras\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from gym import Env\n",
    "\n",
    "from gym.spaces import Discrete, Box\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "# get training envs\n",
    "import pandas as pd\n",
    "all_puzzles = pd.read_csv(\"sudoku.csv\")\n",
    "all_puzzles_df = pd.DataFrame(all_puzzles)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Reshape\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb06a73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class board_player_env(Env):\n",
    "    def __init__(self, game_no = np.random.randint(100000)):\n",
    "        super(board_player_env, self).__init__()\n",
    "        # creating the action space\n",
    "        # self.game_no = game_no\n",
    "        random.seed(time.perf_counter())\n",
    "        game_no = np.random.randint(100000)\n",
    "        \n",
    "        print(\"game being played now:\", game_no)\n",
    "        self.action_space_list = ['up', 'down', 'left', 'right', 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "        self.action_space = Discrete(len(self.action_space_list))\n",
    "        # number of valid actions\n",
    "        self.n_actions = len(self.action_space_list)\n",
    "        self.n_features = 2\n",
    "\n",
    "        # observation space\n",
    "        self.observation_space = Box(low=0, high=9, shape=(9, 9), dtype=int)\n",
    "\n",
    "        # initializing environment\n",
    "        self.puzzle = all_puzzles_df.iloc[game_no]['quizzes']\n",
    "        self.puzzle_arr = np.array(list(self.puzzle), dtype=int).reshape((9, 9))\n",
    "\n",
    "        # initializing perfect solution\n",
    "        self.solution = all_puzzles_df.iloc[game_no]['solutions']\n",
    "        self.solution_arr = np.array(list(self.solution), dtype=int).reshape((9, 9))\n",
    "\n",
    "        # initializing the agent at the first index\n",
    "        self.agent = [0, 0]\n",
    "\n",
    "        # initializing rewards\n",
    "        self.rewards = 0\n",
    "        self.done = False\n",
    "\n",
    "    def render(self):\n",
    "        print(\"\\nsolved now: \\n\")\n",
    "        print(self.puzzle_arr)\n",
    "\n",
    "    def reset(self):\n",
    "        # set the agent at first location\n",
    "        # self.state = [0, 0]\n",
    "        \n",
    "        # maybe it shoulf be there/maybe not\n",
    "        # self.rewards = 0\n",
    "        # self.done = False\n",
    "        return [0, 0]\n",
    "\n",
    "    def step(self, action):\n",
    "\n",
    "        # current agent location\n",
    "        state = self.agent\n",
    "        # print(\"self.agent: \", self.agent)\n",
    "        # print(\"state: \", state[0])\n",
    "        # print(\"action: \", action, type(action))\n",
    "\n",
    "        # rewards at each step\n",
    "        # reward += 1 for reaching empty\n",
    "        # reward += 3 for filling one spot correctly\n",
    "        # reward += 10 for jackpot of full puzzle\n",
    "        # punishment += -2 for filling wrong\n",
    "\n",
    "        # action 0, 1, 2, 3\n",
    "        # imply only some filling in rules of the sudoku game, so that the board doesn't change\n",
    "        if (action == 0):\n",
    "            # print(\"\\nup\")\n",
    "            if(state[0] > 0):\n",
    "                self.agent = [state[0] - 1, state[1]]\n",
    "                if(self.puzzle_arr[state[0] - 1, state[1]] == 0):\n",
    "                    print(\"\\nreward for up\")\n",
    "                    self.rewards += 1\n",
    "                else:\n",
    "                    print(\"\\npunishment for up\")\n",
    "                    self.rewards -= 1\n",
    "\n",
    "        elif (action == 1):\n",
    "            # print(\"\\ndown\")\n",
    "            if(state[0] < 8):\n",
    "                self.agent = [state[0] + 1, state[1]]\n",
    "                if(self.puzzle_arr[state[0] + 1, state[1]] == 0):\n",
    "                    print(\"\\nreward for down\")\n",
    "                    self.rewards += 1\n",
    "                else:\n",
    "                    print(\"\\npunishment for down\")\n",
    "                    self.rewards -= 1\n",
    "\n",
    "        elif (action == 2):\n",
    "            # print(\"\\nleft\")\n",
    "            if(state[1] > 0):\n",
    "                self.agent = [state[0], state[1] - 1]\n",
    "                if(self.puzzle_arr[state[0], state[1] - 1] == 0):\n",
    "                    print(\"\\nreward for left\")\n",
    "                    self.rewards += 1\n",
    "                else:\n",
    "                    print(\"\\npunishment for left\")\n",
    "                    self.rewards -= 1\n",
    "\n",
    "        elif (action == 3):\n",
    "            # print(\"\\nright\")\n",
    "            if(state[1] < 8):\n",
    "                self.agent = [state[0], state[1] + 1]\n",
    "                if(self.puzzle_arr[state[0], state[1] + 1] == 0):\n",
    "                    print(\"\\nreward for right\")\n",
    "                    self.rewards += 1\n",
    "                else:\n",
    "                    print(\"\\npunishment for right\")\n",
    "                    self.rewards -= 1\n",
    "\n",
    "        # action 5 means fill 2\n",
    "        elif (action > 3):\n",
    "            if(self.puzzle_arr[state[0], state[1]] == 0):\n",
    "                if(self.action_space_list[action] == self.solution_arr[state[0], state[1]]):\n",
    "                    self.puzzle_arr[state[0], state[1]] = self.action_space_list[action]\n",
    "                    print(\"\\nreward for action:\", self.action_space_list[action])\n",
    "                    self.rewards += 3\n",
    "                else:\n",
    "                    print(\"\\npunishment for action:\", self.action_space_list[action])\n",
    "                    self.rewards -= 2\n",
    "\n",
    "        if(np.array_equal(self.solution_arr, self.puzzle_arr)):\n",
    "            self.rewards += 10\n",
    "            self.done = True\n",
    "            self.render()\n",
    "            # maybe/maybe not\n",
    "            self.__init__()\n",
    "            \n",
    "\n",
    "        return self.agent, self.rewards, bool(self.done), {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c984c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(states, actions):\n",
    "    model = Sequential()\n",
    "    # model.add(Reshape(target_shape=(actions,), input_shape=states))\n",
    "    model.add(Dense(24, activation='relu', input_shape=(1, 2, )))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(actions))\n",
    "    model.add(Flatten())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d079b90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
    "                  nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-3)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef71e903",
   "metadata": {},
   "source": [
    "```python\n",
    "# do not run in ipynb\n",
    "episodes = 100\n",
    "for run in range(0, 10):\n",
    "    env = board_player_env(run)\n",
    "    for episode in range(0, episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            n_state, reward, done, _ = env.step(action)\n",
    "            env.render()\n",
    "            score += reward\n",
    "        print(\"Episode: \", episode, \"Score:\", score)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed1b38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = board_player_env()\n",
    "states = env.observation_space.shape\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dac01230",
   "metadata": {},
   "source": [
    "# run when error: 'Sequential' has no attribute '_compile_time_distribution_strategy'\n",
    "# and then build model again\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d908776",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(states, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d670fb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83c351d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(learning_rate=1e-3), metrics=['mae'])\n",
    "dqn.fit(board_player_env(), nb_steps=50000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77714e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('dqn_weights.h5f', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d5bfeb",
   "metadata": {},
   "source": [
    "https://blog.paperspace.com/getting-started-with-openai-gym/\n",
    "\n",
    "https://github.com/openai/gym/blob/master/gym/spaces/box.py\n",
    "\n",
    "https://lilianweng.github.io/lil-log/2018/05/05/implementing-deep-reinforcement-learning-models.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
